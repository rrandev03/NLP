### Modeling Assumptions:
I made several assumptions for my implementation of the noisy channel spelling corrector. The most basic one was that I only included as candidates those words which are within one edit of the misspelled word, because this is the case for 80% of typos that occur in the real world. Another assumption was that the probability of an insertion or deletion occurring at the beginning of a word is so low that we could just set it 0, since we didn't have the required '#' unigram or '#letter' bigram counts, respectively. Likewise, if there was a particular substitution, deletion, or addition which did not occur in the provided csv files, the corresponding probabilities were set to 0 for convenience. However, these probabilities are obviously not actually 0. I also assumed that this spelling corrector would only be used for non-real words, and not real-word spelling errors, which is why we did not include as a candidate the actual word that was typed in (candidate with edit distance = 0). Finally, I assumed that all of the words in counts.txt (and thus the potential candidates) are real and that the file contains all possible words that could be a candidate for a given misspelling; in reality it seems like many of them are not real (so we could end up correcting to another non-word).

### Scenarios:
One scenario where the spelling corrector could do better is when considering more of the context would change the outcome of the correction. For example, it corrects "house" to "houses", but in the first example I provided in the pdf, in the context of that specific sentence, "mouse" seems like a more valid correction. To improve our model for these types of scenarios, we could use a bigram or trigram model as the language model instead of a unigram and take in some of the context as the input to the function.
Another scenario where the corrector could be improved is being able to handle real words by including words with an edit distance of 0 and using context to determine if the word should be unchanged or if a different word might actually be better suited based on the words around it. In the second example of the pdf, the corrector changes "the" to "then" and will never just return "the" because the assumption that the input cannot be a real word is baked into the implementation. 
Finally, when I type in a word that is very misspelled (has more than 1 error), my spelling corrector will just return that there are no candidates within one edit and therefore it can't provide a correction (third example). To improve the output in these scenarios, we could just consider all words in the corpus as potential candidates and find the one with the highest probability based on weighted Levenshtein (instead of narrowing down the candidates to words within 1 edit first).
Some scenarios where the corrector works well are also shown in the second part of the pdf.